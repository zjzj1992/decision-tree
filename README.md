# 决策树(ID3)

1、决策树的一个重要任务是为了理解数据中所蕴含的知识信息，因此决策树可以使用不熟悉的数据集合，并从中提取出一系列规则，这些机器根据数据集创建规则的过程，就是机器学习的过程

2、决策树是将空间用超平面进行划分的一种方法，每次分割的时候，都将当前的空间一分为二，这样使得每个叶子节点都是在空间中的一个不相交的区域，在进行决策的时候，会根据输入样本的每一维特征的值，然后一步一步的往下，最后样本会落入N个区域中的一个

3、决策树是一个树形结构(二叉树或非二叉树)

4、结构：决策树模型构成的核心部分是：结点和有向边组成；结点有内部结点和叶节点两种类型；每个内部节点都表示一个特征，而叶节点表示一个类别

5、每个非叶节点都表示一个特征属性，每个非叶节点会根据特征属性上的不同的取值来构造不同的分支，直到最后到达叶节点，而叶节点就是我的分类结果

6、决策树可以如下表示：

![image](https://github.com/zjzj1992/decision-tree-ID3/blob/master/images/xigua1.jpg)

                                            摘自周志华的西瓜书

7、决策树代表实例属性值约束的合取的析取式，意思就是从根节点到每个叶节点都会构成一条路径，而这条路径就是合取，合取的意思就是“and”，在路径上要同时满足某些条件时才能最终得到一个结论，合取针对的是每一条路径；而析取的意思是“or”，它针对的是每条路径，只要满足其中一条，那么就会得到一个分类结论
例如： ((纹理=清晰)(根蒂=蜷缩)）((纹理=清晰)(根蒂=稍蜷)(色泽=乌黑)(触感=硬滑))......

8、决策树实例：
假如我买了一个西瓜，然后我想根据上面提供的决策树来判断我买的这个西瓜到底是好瓜还是坏瓜，那么应该怎么做呢？
首先，我知道我买的这个好瓜的特点是纹理清晰，根蒂硬挺，那么它是好瓜还是坏瓜
根据上面的决策树可以发现，决策树首先根据纹理来进行判断，而我买的这个瓜的纹理是清晰的，所以根据纹理的不同取值会进入不同的分支，这里是进入清晰的那个分支，然后根据决策树，还要再判断根蒂的状况，而我买的这个瓜根蒂是硬挺的，所以根据这个取值就进入了硬挺的那个分支，此时就达到了叶节点，而该叶节点的类别就是我的预测结果，结果显示纹理清晰、根蒂硬挺的瓜是一个坏瓜

9、那么话又说回来了，前面提到的案例都是根据决策树做出的判断，可是决策树是怎么生成的呢？决策树最终的目的就是希望可以将数据分的更“纯”，意思是，根据不同的决策路径，可以将每个样本正确的分到不同的类别中，这就是决策树的目的。要构建这样的分类决策树有三种主要的算法，分别是ID3、C4.5和CART。根据这三种不同的算法可以构建出不同的决策树。

10、下面介绍这三种不同的算法的实现形式：

（1）决策树可以看成一个if-then规则的集合，从根节点到每个叶节点都可以构成一条规则
         并且预测的实例一定都可以被一条路径或一条规则所覆盖

（2）决策树在构造的过程中，如果选择每层的特征是非常重要的。如果选择好了每层的特征，那么树也就构建起来了

（3）决策树构建的关键其实就是选择可以最优划分属性，并且希望划分之后可以让分支结点变得更纯一些，那么纯度的度量方法的不同，也就导致了算法的不同，其中最
常用的算法是ID3和C4.5

11、介绍ID3：

（1）上面说了，希望划分之后让结点更纯，可是要怎么样衡量这个纯呢？最常用的度量方式就是使用“信息熵”，“信息熵”是度量样本集合不确定性的最常用的指标
在ID3算法中，我们采取信息增益这个量来作为纯度的度量。我们选取使得信息增益最大的特征进行划分。
上句话提到了信息增益，信息增益的意思是：按某个特征划分的前后信息熵的变化情况就是信息增益，一般选择信息增益最大的特征作为分裂属性，因为只有选择这样的特征才能带来最显著的由不纯到纯的这么一个效果，而这个效果正是我需要的。
条件熵：代表数据在某一条件下随机变量的复杂程度(混乱程度)
信息增益：信息熵-条件熵

（2）当样本集合D中第k类样本所占的比例为pk，则D的信息熵定义为：

![image](https://github.com/zjzj1992/decision-tree-ID3/blob/master/images/xinxishang.jpg)

•离散属性 a 有 V 个可能的取值 {a1,a2,…,aV}；样本集合中，属性 a 上取值为 av 的样本集合，记为 Dv。

•用属性 a 对样本集 D 进行划分所获得的“信息增益”

![image](https://github.com/zjzj1992/decision-tree-ID3/blob/master/images/zengyi.jpg)

（3）信息增益表示根据某属性划分之后而使得样本集合不确定性减少的程度

（4）现在应该知道该怎么选择特征列，就是如果选择一个特征之后，信息增益是最大的，也就是说信息的不确定性减少的程度最大，那么就可以选择这个特征
举例说明，这是西瓜书上很好的例子：
假如现在给你一些关于西瓜的数据，表格如下：

![image](https://github.com/zjzj1992/decision-tree-ID3/blob/master/images/data.jpg)

然后根据这个表格里的信息，可以得到的信息是：好瓜占的比例是8/17，坏瓜占的比例是9/17
所以根据这两个数据可以计算出信息熵，公式为：

![image](https://github.com/zjzj1992/decision-tree-ID3/blob/master/images/xiguashang.jpg)

然后可以开始根据不同的特征去计算条件熵了，然后选择一个信息增益最大的特征作为当前的分裂属性。
在该数据中，关于瓜的属性有色泽、根蒂、敲声、纹理、脐部和触感，我们可以先计算一下色泽的条件熵：
D1(色泽=青绿) = {1, 4, 6, 10, 13, 17}，正例 3/6，反例 3/6
D2(色泽=乌黑) = {2, 3, 7, 8, 9, 15}，正例 4/6，反例 2/6
D3(色泽=浅白) = {5, 11, 12, 14, 16}，正例 1/5，反例 4/5
然后就可以计算出三个分支的信息熵了：

![image](https://github.com/zjzj1992/decision-tree-ID3/blob/master/images/xiguashang2.jpg)

然后根据公式就可以计算出当前色泽这个特征对应的信息增益是多少了，公式为：

![image](https://github.com/zjzj1992/decision-tree-ID3/blob/master/images/xiguashang3.jpg)

同理，我们可以计算出关于其他属性对应的信息增益，分别如下：

![image](https://github.com/zjzj1992/decision-tree-ID3/blob/master/images/xiguashang4.jpg)

根据结果发现信息增益最大的是纹理，于是我们可以选择纹理作为当前数据的分裂属性，分裂结束后，会得到一份新的数据：

![image](https://github.com/zjzj1992/decision-tree-ID3/blob/master/images/xiguashang5.jpg)

然后在三个不同的分支下，我们可以递归的使用上面的方法去计算信息增益，然后选择增益最大的那个特征作为分裂属性，比如在清晰分支中，还剩下的属性有，色泽、根蒂、敲声、脐部和触感，我们可以基于该分支下的数据继续计算这几个属性对应的信息增益，从而选择出最合适的特征。

![image](https://github.com/zjzj1992/decision-tree-ID3/blob/master/images/xiguashang6.jpg)

如果信息增益相等的话，那么可以随意选择一个作为分裂属性，就这样的递归的通过计算选择特征直到最终构建出决策树

![image](https://github.com/zjzj1992/decision-tree-ID3/blob/master/images/xiguashang7.jpg)

对于新样本，通过遍历决策树就可以得到关于该样本的预测类别了

（5）ID3算法存在一个问题，就是该算法计算出来的信息增益对可取值数目较多的属性有所偏好。
现在我们假设数据集中的编号也作为候选的待划分属性，因为每个样本都有一个不同的编号，所以如果按照编号进行划分的话，叶子节点只会有一个，不存在混乱的情况，纯度非常高，也就是说，给我一个新样本，只要告诉我编号，我就可以得出决策结果，其他特征都是没用的，但是这样的决策树是不具备泛化能力的，所以针对该种情况就引入了第二种计算方式——信息增益率，通过信息增益率选择最优划分属性
